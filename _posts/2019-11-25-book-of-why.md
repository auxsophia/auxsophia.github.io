---
layout: post
title:  "Book of Why"
permalink: /bookOfWhy/
date:   2019-11-21 21:36:30 -0700
categories: books
---

# [THE BOOK OF WHY: THE NEW SCIENCE OF CAUSE AND EFFECT](http://bayes.cs.ucla.edu/WHY/)
### JUDEA PEARL AND DANA MACKENZIE

New York: Basic Books, Published May 15, 2018

---

Paradoxes arise from rules in one domain improperly applied to another.

Data is amenable to probability and statistics. Causality is from an observer.

He talks about the famous [Monty hall problem](https://en.wikipedia.org/wiki/Monty_Hall_problem). He argues the essential problem behind the confusion that the probabilities are still 1/3rd after a door has been open ignores how the data is generated. Merely looking at the data makes us believe the probability of 1/3rd, but knowing how the data is generated, the gameshow host picking a door which doesn't hide the car, represents important information about the remaining door. That's why we should switch our choice since the remaining door has a 2/3rds chance of being correct.

---

Big data allows us to generalize from many studies to the real world.

**Transportability**: generalizing inference from many studies to the real world. Previously each domain had to determine what was a valid/invalid inference.

With big data, causal diagrams, and the current techniques developed, we can now adjust for confounding variables across studies.

---

### Strong AI

He's a compatibilist on free will.

To think in terms of intents and subjective options is a hallmark of free will even if the neurological description is determined. The cognitive description, the subjective sense, allows us to reason about intent and the internal factors leading to our decision. He says normative moral questions function like counter-factual statements. We're talking about a better outcome if another action had been taken. Telling a machine it ought to have acted differently means it should consider what a possible outcome would have been if it acted differently and adjust the "software packages" which lead to the actual outcome towards the better outcome. This is subjective since the internal paths/representations are different for each system, neural connections, artificial or natural.

A machine would need a memory of past activations to answer questions about previous actions. Why did you do that? We might get answers like, "because the alternative proved less attractive," "I wish I knew why," or "because that's the way you programmed me," depending on the type of action. He believes a machine with a sense of agency consists of 3 parts:
- a causal model of the world
- a causal model of its own software no matter how superficial  
- a memory of how intents in its own mind correspond to events in the outside world

We have an intention generator in our mind. When we experiment and keep a record of our intentions and its effects in the world, we gain an understanding of our intention generator. We begin to have moral responsibility when we adjust our own software.

Strong AI is a realizable promise!

The AI might even feel it has free will. We can have machines that recognize good and evil once we program in self reflection with an understanding of intents and causal effects, empathy, long term prediction, and self-restraint. It won't be rule based or prescriptive. There is no reason to refrain from building machines which are better able to distinguish good and evil than we are, better able to resist temptation, better able to assign guilt and credit. We will be able to learn from the machine.
